# Database Activity Monitoring and Auditing Application

## Project Description
This project is developed to monitor operations on critical databases in a corporate environment and, if necessary, generate alerts and reports. It has an architecture that collects events from different database sources, classifies them, and indexes them in Elasticsearch.

## Project Requirements
To run the project, the following software and tools must be installed:
- Docker and Docker Compose
- .NET SDK 8.0 or higher

## Installation Instructions
1. Navigate to the project directory.
2. Start all services using Docker Compose by running the following command:
   ```bash
   docker-compose up -d
   ```
3. Verify that the services are running by checking the Docker containers:
   ```bash
   docker ps
   ```

## Docker Compose Usage
The Docker Compose file includes dependencies such as PostgreSQL, Kafka, Zookeeper, and Elasticsearch. All services are defined in the `docker-compose.yml` file.

## Endpoint Information
- **Collector Service**: Collects database activities and sends them to Kafka.
- **Processor Service**: Processes events from Kafka and indexes them in Elasticsearch.
- **Config Service**: Manages configurations and rules in PostgreSQL.
- **Reporting Service**: Queries and reports data from Elasticsearch.

## Detailed Endpoint Descriptions

### Collector Service
- **Endpoint**: `POST /api/collect`
- **Description**: Collects database activities and sends them to Kafka.
- **Request Body**:
  ```json
  {
    "eventType": "SELECT",
    "username": "admin",
    "databaseName": "CustomerDB"
  }
  ```
- **Response**: Returns HTTP 200 on a successful request.

### Processor Service
- **Endpoint**: `POST /api/process`
- **Description**: Processes events from Kafka and indexes them in Elasticsearch.
- **Response**: Returns HTTP 200 on a successful request.

### Config Service
- **Endpoint**: `GET /api/rules`
- **Description**: Lists all rules.
- **Response Body**:
  ```json
  [
    {
      "id": "...",
      "name": "Critical Rule",
      "isActive": true
    }
  ]
  ```

### Reporting Service
- **Endpoint**: `GET /api/reports/critical-events?startDate=...&endDate=...`
- **Description**: Lists critical events within the specified date range.
- **Response Body**:
  ```json
  [
    {
      "eventId": "...",
      "eventType": "LOGIN",
      "timestamp": "...",
      "username": "admin",
      "databaseName": "CustomerDB",
      "severity": "High"
    }
  ]
  ```

## Sample Data Scenario
You can provide a simple data generator within the Collector Service or with a different project to continuously or periodically send 10-20 sample audit events to Kafka. The Processor Service can then capture these events and index them in Elasticsearch.

## Testing and Verification
To verify that the project is working, follow these steps:
1. Verify that the Docker containers are running:
   ```bash
   docker ps
   ```
2. Use Postman or a similar tool to test the API endpoints.
3. Check the indexed data in Elasticsearch.

## Sample Log Outputs
Below are sample outputs of logs generated by the services:
```
[INFO] CollectorService: Produced event: SELECT
[ERROR] ProcessorService: Error processing event
```

## Development Environment Setup
To run the project in a development environment, follow these steps:
1. Clone the project:
   ```bash
   git clone https://github.com/ogunkirikci/dbTrackerApp.git
   ```
2. Navigate to the project directory:
   ```bash
   cd repo
   ```
3. Install the necessary dependencies:
   ```bash
   dotnet restore
   ```
4. Run the project:
   ```bash
   dotnet run
   ```
